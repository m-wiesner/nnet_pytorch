diff --git a/__init__.py b/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/pychain/__init__.py b/pychain/__init__.py
index 890d65b..84a54e6 100644
--- a/pychain/__init__.py
+++ b/pychain/__init__.py
@@ -1,2 +1 @@
-from .loss import *
 from .graph import *
diff --git a/pychain/chain.py b/pychain/chain.py
new file mode 100644
index 0000000..17c0b64
--- /dev/null
+++ b/pychain/chain.py
@@ -0,0 +1,83 @@
+# Copyright       2019 Yiwen Shao
+#                 2020 Yiming Wang
+
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+
+#  http://www.apache.org/licenses/LICENSE-2.0
+
+# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
+# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
+# MERCHANTABLITY OR NON-INFRINGEMENT.
+# See the Apache 2 License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+import torch.nn as nn
+from .graph import ChainGraphBatch
+import pychain_C
+
+
+class ChainFunction(torch.autograd.Function):
+    @staticmethod
+    def forward(ctx, input, input_lengths, graphs, leaky_coefficient=1e-5):
+        input = input.contiguous().clamp(-30, 30)  # clamp for both the denominator and the numerator
+        B = input.size(0)
+        if B != graphs.batch_size:
+            raise ValueError(
+                "input batch size ({}) does not equal to graph batch size ({})"
+                .format(B, graphs.batch_size)
+            )
+        packed_data = torch.nn.utils.rnn.pack_padded_sequence(
+            input, input_lengths, batch_first=True,
+        )
+        batch_sizes = packed_data.batch_sizes
+        input_lengths = input_lengths.cpu()
+        if not graphs.log_domain:  # usually for the denominator
+            exp_input = input.exp()
+            objf, input_grad, ok = pychain_C.forward_backward(
+                graphs.forward_transitions,
+                graphs.forward_transition_indices,
+                graphs.forward_transition_probs,
+                graphs.backward_transitions,
+                graphs.backward_transition_indices,
+                graphs.backward_transition_probs,
+                graphs.leaky_probs,
+                graphs.initial_probs,
+                graphs.final_probs,
+                graphs.start_state,
+                exp_input,
+                batch_sizes,
+                input_lengths,
+                graphs.num_states,
+                leaky_coefficient,
+            )
+        else:  # usually for the numerator
+            objf, log_probs_grad, ok = pychain_C.forward_backward_log_domain(
+                graphs.forward_transitions,
+                graphs.forward_transition_indices,
+                graphs.forward_transition_probs,
+                graphs.backward_transitions,
+                graphs.backward_transition_indices,
+                graphs.backward_transition_probs,
+                graphs.initial_probs,
+                graphs.final_probs,
+                graphs.start_state,
+                input,
+                batch_sizes,
+                input_lengths,
+                graphs.num_states,
+            )
+            input_grad = log_probs_grad.exp()
+
+        ctx.save_for_backward(input_grad)
+        return objf.sum()
+
+    @staticmethod
+    def backward(ctx, objf_grad):
+        input_grad, = ctx.saved_tensors
+        input_grad = torch.mul(input_grad, objf_grad)
+
+        return input_grad, None, None, None
diff --git a/pytorch_binding/src/chain-computation.cc b/pytorch_binding/src/chain-computation.cc
index d53a03f..36edf10 100644
--- a/pytorch_binding/src/chain-computation.cc
+++ b/pytorch_binding/src/chain-computation.cc
@@ -226,7 +226,8 @@ torch::Tensor ChainComputation::ComputeTotLogLike() {
   // as alpha_frame_log_tot is padded with 0.0, the sum below is fine
   tot_log_prob_.copy_(alpha_frame_log_tot.sum(1) + last_frame_alpha_dash_sum.log()); // B
   tot_prob_.copy_(tot_log_prob_.exp()); // B
-  return tot_log_prob_.sum();
+  //return tot_log_prob_.sum();
+  return tot_log_prob_;
 }
 
 void ChainComputation::BetaDashLastFrame() {
