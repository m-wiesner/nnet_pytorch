From 640a85e4455a72f092552e8f1475681ca77c87c7 Mon Sep 17 00:00:00 2001
From: Matthew Wiesner <wiesner@jhu.edu>
Date: Thu, 29 Jul 2021 12:25:20 -0400
Subject: [PATCH] nnet_pytorch updates

---
 pychain/__init__.py                      |  1 -
 pychain/chain.py                         | 88 ++++++++++++++++++++++++++++++++
 pytorch_binding/src/chain-computation.cc |  3 +-
 3 files changed, 90 insertions(+), 2 deletions(-)
 create mode 100644 pychain/chain.py

diff --git a/pychain/__init__.py b/pychain/__init__.py
index 890d65b..84a54e6 100644
--- a/pychain/__init__.py
+++ b/pychain/__init__.py
@@ -1,2 +1 @@
-from .loss import *
 from .graph import *
diff --git a/pychain/chain.py b/pychain/chain.py
new file mode 100644
index 0000000..f7f7174
--- /dev/null
+++ b/pychain/chain.py
@@ -0,0 +1,88 @@
+# Copyright       2019 Yiwen Shao
+#                 2020 Yiming Wang
+
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+
+#  http://www.apache.org/licenses/LICENSE-2.0
+
+# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
+# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
+# MERCHANTABLITY OR NON-INFRINGEMENT.
+# See the Apache 2 License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+import torch.nn as nn
+from .graph import ChainGraphBatch
+import pychain_C
+
+
+class ChainFunction(torch.autograd.Function):
+    @staticmethod
+    @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
+    def forward(ctx, input, input_lengths, graphs, leaky_coefficient=1e-5, clamp=True):
+        input = input.contiguous()
+        if clamp:
+            input = input.clamp(-30, 30)  # clamp for both the denominator and the numerator
+        B = input.size(0)
+        if B != graphs.batch_size:
+            raise ValueError(
+                "input batch size ({}) does not equal to graph batch size ({})"
+                .format(B, graphs.batch_size)
+            )
+        
+        input_lengths = input_lengths.cpu()
+        packed_data = torch.nn.utils.rnn.pack_padded_sequence(
+            input, input_lengths, batch_first=True,
+        )
+        batch_sizes = packed_data.batch_sizes
+        if not graphs.log_domain:  # usually for the denominator
+            exp_input = input.exp()
+            objf, input_grad, ok = pychain_C.forward_backward(
+                graphs.forward_transitions,
+                graphs.forward_transition_indices,
+                graphs.forward_transition_probs,
+                graphs.backward_transitions,
+                graphs.backward_transition_indices,
+                graphs.backward_transition_probs,
+                graphs.leaky_probs,
+                graphs.initial_probs,
+                graphs.final_probs,
+                graphs.start_state,
+                exp_input,
+                batch_sizes,
+                input_lengths,
+                graphs.num_states,
+                leaky_coefficient,
+            )
+        else:  # usually for the numerator
+            objf, log_probs_grad, ok = pychain_C.forward_backward_log_domain(
+                graphs.forward_transitions,
+                graphs.forward_transition_indices,
+                graphs.forward_transition_probs,
+                graphs.backward_transitions,
+                graphs.backward_transition_indices,
+                graphs.backward_transition_probs,
+                graphs.initial_probs,
+                graphs.final_probs,
+                graphs.start_state,
+                input,
+                batch_sizes,
+                input_lengths,
+                graphs.num_states,
+            )
+            input_grad = log_probs_grad.exp()
+
+        ctx.save_for_backward(input_grad)
+        return objf.sum()
+
+    @staticmethod
+    @torch.cuda.amp.custom_bwd
+    def backward(ctx, objf_grad):
+        input_grad, = ctx.saved_tensors
+        input_grad = torch.mul(input_grad, objf_grad)
+
+        return input_grad, None, None, None, None
diff --git a/pytorch_binding/src/chain-computation.cc b/pytorch_binding/src/chain-computation.cc
index d53a03f..36edf10 100644
--- a/pytorch_binding/src/chain-computation.cc
+++ b/pytorch_binding/src/chain-computation.cc
@@ -226,7 +226,8 @@ torch::Tensor ChainComputation::ComputeTotLogLike() {
   // as alpha_frame_log_tot is padded with 0.0, the sum below is fine
   tot_log_prob_.copy_(alpha_frame_log_tot.sum(1) + last_frame_alpha_dash_sum.log()); // B
   tot_prob_.copy_(tot_log_prob_.exp()); // B
-  return tot_log_prob_.sum();
+  //return tot_log_prob_.sum();
+  return tot_log_prob_;
 }
 
 void ChainComputation::BetaDashLastFrame() {
-- 
1.8.3.1

